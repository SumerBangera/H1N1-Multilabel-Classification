# Loading packages required

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


# Verfication functions

def check_na(df):
    round(df.isna().sum()*100/len(df),3)

def data_summary(df):
    for col in df:
        print(col,"-----  ", 
              df[col].nunique(),"-----  ",
              df[col].unique())    

def create_dummies(df, col_type):
    df = pd.get_dummies(df, columns = col_type, dummy_na=False, drop_first=True) 
    # Important to use drop_first=True as created dummies lead to high multicollinearity leading to Dummy Variable Trap
    
    return df

def scaling_features(df, col_type):
    for col in col_type:
        df[col] = df[col]/df[col].max()
    
    return df


# Function to impute NAs with mean value

def na_impute_mean(df, col_type):
    df[col_type] = df[col_type].fillna(df[col_type].mean())
    
    return df


# To view feature-pairs with highest correlation

def get_redundant_pairs(df):
    '''Get diagonal and lower triangular pairs of correlation matrix'''
    pairs_to_drop = set()
    cols = df.columns
    for i in range(0, df.shape[1]):
        for j in range(0, i+1):
            pairs_to_drop.add((cols[i], cols[j]))
    return pairs_to_drop

def get_top_abs_correlations(df, n=5):
    corr_pairs = df.corr().abs().unstack()
    labels_to_drop = get_redundant_pairs(df)
    corr_pairs = corr_pairs.drop(labels=labels_to_drop).sort_values(ascending=False)
    
    return corr_pairs[0:n]


# To delete highly correlated features

def del_high_corr_features(df, threshold = 0.95):
    corr_matrix = df.corr().abs()
    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))
    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]
    df.drop(df[to_drop], axis=1)
    
    return df

def preprocessing_data(df):
    
    # creating dummies for nominal columns
    df = create_dummies(df, nominal_cols)
   
    # replacing transformed ordinal_cols_cat into the dataframe
    df.replace(ordinal_cols_cat_dict, inplace=True)

    # imputing NA values with "mean"
    
    df = na_impute_mean(df, binary_cols)
    df = na_impute_mean(df, ordinal_cols)
    df = na_impute_mean(df, ordinal_cols_cat)    
    
    # rescaling columns
    
    scaling_features(df, ordinal_cols)
    scaling_features(df, ordinal_cols_cat)
    
    df.rename(columns={'education_< 12 Years':"education_less_12 Years"}, inplace=True)
    df.rename(columns={'census_msa_MSA, Principle City':"census_msa_MSA_Principle City"}, inplace=True)    
    
    return df


# Separating columns based on their type

# Variables that do not need dummies as they are already binary
binary_cols = [ "behavioral_antiviral_meds", 
                "behavioral_avoidance", 
                "behavioral_face_mask", 
                "behavioral_wash_hands", 
                "behavioral_large_gatherings", 
                "behavioral_outside_home", 
                "behavioral_touch_face", 
                "doctor_recc_h1n1", 
                "doctor_recc_seasonal", 
                "chronic_med_condition", 
                "child_under_6_months", 
                "health_worker", 
                "health_insurance"
                ]

# Nominal variables
nominal_cols = ["race",
                "sex",
                "marital_status",
                "rent_or_own",
                "employment_status",
                "education",
                "hhs_geo_region",
                "census_msa",
                "household_adults",
                "household_children",
                "employment_industry",
                "employment_occupation"
                ]

# Numeric ordinal variables
ordinal_cols = ["h1n1_concern", 
                "h1n1_knowledge", 
                "opinion_h1n1_vacc_effective",
                "opinion_h1n1_risk", 
                "opinion_h1n1_sick_from_vacc",
                "opinion_seas_vacc_effective",
                "opinion_seas_risk", 
                "opinion_seas_sick_from_vacc"
                ]

# Categorical ordinal variables
ordinal_cols_cat_dict = {"age_group":{"18 - 34 Years": (18+34)/2, "35 - 44 Years": (35+44)/2, 
                                 "45 - 54 Years": (45+54)/2,"55 - 64 Years": (45+54)/2,
                                 "65+ Years": 65},              
                
                         "income_poverty": {"Below Poverty": 22025, 
                                            "<= $75,000, Above Poverty": (22025+75000)/2, 
                                            "> $75,000": 75000}
                   }
ordinal_cols_cat = ["age_group", "income_poverty"]


# Model building

# Packages required
from sklearn import metrics
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.multioutput import MultiOutputClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost.sklearn import XGBClassifier
import warnings
warnings.filterwarnings("ignore")
RANDOM_SEED = 6  

# Creating train and validation datasets
X_train, X_val, y_train, y_val = train_test_split(train_df, train_labels, 
                                                  test_size = 0.2, 
                                                  shuffle = True,    
                                                  stratify=train_labels,
                                                  random_state=RANDOM_SEED)

# Initializing Hyperparameters of the model
MultinomialNB_param = {
    'alpha': [0, 1, 10, 20, 50, 100],   # tried [0,1, 1, 10]
    'fit_prior': [True, False],
}

LogisticRegression_param = {
    "C": [0.18, 0.2, 0.22, 0.3, 0.5],        # tried [0.001, 0.003, 0.01, 0.03, 0.1, 0.15, 0.2, 0.22, 0.25 0.3, 0.4, 0.8 1, 3, 10] 
    "max_iter": [50, 100, 110, 125, 140],    # tried [25, 50, 100, 125, 150, 175, 200,250]
    "penalty": ["l2"],                       # tried ["l1", "l2"]
}

DecisionTreeClassifier_param = {
    "criterion": ["gini", "entropy"],
    "max_depth": [40, 80, 100],    
    "max_features": ["None","auto",'sqrt', "log2"],      
    "min_samples_leaf": [3,5],
    "min_samples_split": [2,3]
}

RandomForestClassifier_param = {
    "bootstrap": [True],   
    "n_estimators": [300, 400],                          # tried [100, 200, 250, 300]
    "max_depth": [40, 80, 100],                          # tried already with [10, 30, 40, 50, 60, 80]
    "max_features": ["None","auto",'sqrt', "log2"],      # tried ["auto",'sqrt']
    "min_samples_leaf": [3,5],                           # tried [2, 3, 4]        # 3 seems to be the best
    "min_samples_split": [2,3],                          # started with [2, 3, 5]   # 3 seems to be the best
}

XGBClassifier_param = {
    "n_estimators": [50, 100, 200, 300],                 
    "objective": ["binary:logistic"],
    "min_child_weight": [1],
    "gamma": [0.5, 0.8, 1.0],                            
    "subsample": [0.6, 0.8, 1.0],                        
    "colsample_bytree": [0.8, 1.0],                      
    "max_depth": [1, 3, 5],
}


# Creating functions for model building

def plot_roc(y_true, y_score, label_name, ax):
    fpr, tpr, thresholds = roc_curve(y_true, y_score)
    ax.plot(fpr, tpr)
    ax.plot([0, 1], [0, 1], color='grey', linestyle='--')
    ax.set_ylabel('TPR')
    ax.set_xlabel('FPR')
    ax.set_title(
        f"{label_name}: AUC = {roc_auc_score(y_true, y_score):.4f}"
    )

def best_model_fit(X_train, y_train, X_val, y_val, clf, param):
    
    param = {f'estimator__{k}': v for k, v in param.items()}
    
    print(param)
    
    model = GridSearchCV(estimator = MultiOutputClassifier(clf), param_grid = param, 
                         cv=5, scoring="roc_auc", verbose = 0)
    model.fit(X_train, y_train)

    print("Best cross validation score is:", model.best_score_)
    print(model.best_params_)    
    
    preds = model.predict_proba(X_val)
    
    print("validation_probas[0].shape", preds[0].shape)
    print("validation_probas[1].shape", preds[1].shape)

    y_preds = pd.DataFrame({ "h1n1_vaccine": preds[0][:, 1],
                             "seasonal_vaccine": preds[1][:, 1],
                            },
                            index = y_val.index
                           )
    print("y_preds.shape:", y_preds.shape)
    y_preds.head()
    
    fig, ax = plt.subplots(1, 2, figsize=(7, 3.5))
    plot_roc(
        y_val['h1n1_vaccine'], 
        y_preds['h1n1_vaccine'], 
        'h1n1_vaccine',
        ax=ax[0]
    )
    plot_roc(
        y_val['seasonal_vaccine'], 
        y_preds['seasonal_vaccine'], 
        'seasonal_vaccine',
        ax=ax[1]
    )
    fig.tight_layout()
    
    print("AUC-ROC Score:", roc_auc_score(y_val, y_preds))
    
    return model

# Model fitting
multi_nb_model = best_model_fit(X_train, y_train, X_val, y_val, MultinomialNB(), MultinomialNB_param)


logit_model = best_model_fit(X_train, y_train, X_val, y_val, LogisticRegression(), LogisticRegression_param)


dt_model = best_model_fit(X_train, y_train, X_val, y_val, DecisionTreeClassifier(), DecisionTreeClassifier_param)


rf_model = best_model_fit(X_train, y_train, X_val, y_val, RandomForestClassifier(), RandomForestClassifier_param)


X_train = X_train.values 
X_val = X_val.values
xgb_model = best_model_fit(X_train, y_train, X_val, y_val, XGBClassifier(), XGBClassifier_param)


# Test data and Submission file - for Logistic Regression and RandomForest 

logit_model = best_model_fit(X_train, y_train, X_val, y_val, LogisticRegression(), LogisticRegression_param)
test_probas = logit_model.predict_proba(test_df)
test_probas

rf_model = best_model_fit(X_train, y_train, X_val, y_val, RandomForestClassifier(), RandomForestClassifier_param)
test_probas = rf_model.predict_proba(test_df)
test_probas


submission_df = pd.read_csv("submission_format.csv", 
                            index_col="respondent_id")
submission_df.head()


# Make sure we have the rows in the same order
np.testing.assert_array_equal(test_df.index.values, 
                              submission_df.index.values)

# Save predictions to submission data frame
submission_df["h1n1_vaccine"] = test_probas[0][:, 1]
submission_df["seasonal_vaccine"] = test_probas[1][:, 1]
submission_df.head()


# Creating Submission File

submission_df.to_csv('submission.csv', index=True)

